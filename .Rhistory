for (k in 1:length(knn_sig))
{
eval_point <- x_test
idx_mat <- knnx.index(x_train, eval_point, k)
for (i in 1:eval_n)
{
yhat[i] <- mean(y_train[idx_mat[i,]])
}
knn_sig[k] <- mean((yhat - y_test)^2)
}
plot(knn_sig)
eval_test_n = 10000
library(FNN)
ii = 1
eval_n <- 100
# Generate Train Data
set.seed(ii)
x_train <- sort(rnorm(eval_n))
y_train <- 3 + x_train ^ 2 + rnorm(eval_n)
yhat <- rep(0, eval_n)
knn_sig <- rep(0, 50)
eval_test_n = 10000
# Generate Test Data
x_test <- sort(rnorm(eval_test_n))
y_test <- 3 + x_test ^ 2 + rnorm(eval_test_n)
for (k in 1:length(knn_sig))
{
eval_point <- x_test
idx_mat <- knnx.index(x_train, eval_point, k)
for (i in 1:eval_test_n)
{
yhat[i] <- mean(y_train[idx_mat[i,]])
}
knn_sig[k] <- mean((yhat - y_test)^2)
}
plot(knn_sig)
library(FNN)
ii = 1
eval_n <- 100
# Generate Train Data
set.seed(ii)
x_train <- sort(rnorm(eval_n))
y_train <- 3 + x_train ^ 2 + rnorm(eval_n)
yhat <- rep(0, eval_n)
knn_sig <- rep(0, 50)
eval_test_n = 100
# Generate Test Data
x_test <- sort(rnorm(eval_test_n))
y_test <- 3 + x_test ^ 2 + rnorm(eval_test_n)
for (k in 1:length(knn_sig))
{
eval_point <- x_test
idx_mat <- knnx.index(x_train, eval_point, k)
for (i in 1:eval_test_n)
{
yhat[i] <- mean(y_train[idx_mat[i,]])
}
knn_sig[k] <- mean((yhat - y_test)^2)
}
plot(knn_sig)
library(FNN)
ii = 1
eval_n <- 1000
# Generate Train Data
set.seed(ii)
x_train <- sort(rnorm(eval_n))
y_train <- 3 + x_train ^ 2 + rnorm(eval_n)
yhat <- rep(0, eval_n)
knn_sig <- rep(0, 50)
eval_test_n = 1000
# Generate Test Data
x_test <- sort(rnorm(eval_test_n))
y_test <- 3 + x_test ^ 2 + rnorm(eval_test_n)
for (k in 1:length(knn_sig))
{
eval_point <- x_test
idx_mat <- knnx.index(x_train, eval_point, k)
for (i in 1:eval_test_n)
{
yhat[i] <- mean(y_train[idx_mat[i,]])
}
knn_sig[k] <- mean((yhat - y_test)^2)
}
plot(knn_sig)
plot(knn_sig, type = 'b', color = 'lightblue')
knn_sig <- seq(1,100,by = 5)
knn_sig <- seq(1,100,by = 5)
library(FNN)
ii = 1
eval_n <- 1000
# Generate Train Data
set.seed(ii)
x_train <- sort(rnorm(eval_n))
y_train <- 3 + x_train ^ 2 + rnorm(eval_n)
yhat <- rep(0, eval_n)
knn_sig <- seq(1,100,by = 5)
eval_test_n = 1000
# Generate Test Data
x_test <- sort(rnorm(eval_test_n))
y_test <- 3 + x_test ^ 2 + rnorm(eval_test_n)
for (k in 1:length(knn_sig))
{
eval_point <- x_test
idx_mat <- knnx.index(x_train, eval_point, k)
for (i in 1:eval_test_n)
{
yhat[i] <- mean(y_train[idx_mat[i,]])
}
knn_sig[k] <- mean((yhat - y_test)^2)
}
plot(knn_sig, type = 'b', color = 'lightblue')
plot(knn_sig, type = 'b', col = 'lightblue')
knn_sig
seq(1,100, by = 5)
kvec <- seq(1,100, by = 5)
library(FNN)
ii = 1
eval_n <- 500
# Generate Train Data
set.seed(ii)
x_train <- sort(rnorm(eval_n))
y_train <- 3 + x_train ^ 2 + rnorm(eval_n)
yhat <- rep(0, eval_n)
kvec <- seq(1,100, by = 5)
error <- rep(0, length(kvec))
eval_test_n = 1000
# Generate Test Data
x_test <- sort(rnorm(eval_test_n))
y_test <- 3 + x_test ^ 2 + rnorm(eval_test_n)
for (k in 1:length(knn_sig))
{
eval_point <- x_test
idx_mat <- knnx.index(x_train, eval_point, k)
for (i in 1:eval_test_n)
{
yhat[i] <- mean(y_train[idx_mat[i,]])
}
knn_sig[k] <- mean((yhat - y_test)^2)
}
plot(knn_sig, type = 'b', col = 'lightblue')
plot(knn_sig, type = 'b', col = 'darkblue')
result
## description
## investigate the variance of the proposed estimator
rm(list = ls()); gc()
#setwd("E:\\rank consistency\\simulation\\code\\")
#setwd("C:/Users/uos_stat/Dropbox/A rank-consistency/prog/temp")
setwd("C:/Users/Jeon/Documents/GitHub/RankConsistency")
require('glmnet')
library(igraph)
library(ggplot2)
library(dplyr)
source('./lib/sim.R')
##########################################################################################
max.k = 10
p = 10
kn <- 7  ## d
rn <- 3   ## n_s
df = 1    ## t분포 자유도
counter = 1
sim.iter = 200    ## 전체 simulation 과정 반복 수
source('./lib/exe-2.R')
tn_vec = c(500,5000,50000)
cor.naive_list = list()
cor.r_list = list()
k_fold = 5
tn_i = 3
tn = tn_vec[tn_i]  ## tn 정의 (전체 rank pair의 수.)
cat ('total n:' , tn , '\n')
cor.naive<- rep(0,sim.iter) ## BT를 이용한 kendall's tau 저장하는 벡터
cor.r <- matrix(0,sim.iter,max.k) ## gBT를 이용한 kendall's tau 저장하는 벡터
ii = 1
if (ii %% 10 == 0)  cat(' ',ii,'-th iteration\n')
set.seed(ii+123) ## set seed
### generating number of comparison using dmat
### output: Qmat (n_jk matrix)
dmat1 <- dmat ## dmat : {q_jk} matrix
u.idx <- which( dmat > 0) ## q_jk가 0보다 큰 index
sel.u.idx<- sample(u.idx, kn) ## d개를 sampling함.
dmat1[sel.u.idx]  <- 0  ## d개의 선택된 q_jk에는 0을 대입(어차피 해당 n_jk은 n_s로 고정할 것이므로)
dmat1 <- dmat1/sum(dmat1) ## dmat1을 prob distribution으로 만들어주기 위해 normalize.
d.sample <- drop (rmultinom(1, tn-rn*kn, prob = c(dmat1)))  ## n_jk 만들기
d.sample[sel.u.idx] <- rn ## d개의 선택된 n_jk에 n_s를 대입
dmat1 <- matrix(d.sample, p , p)  ## matrix 형태로 만들어줌.
Qmat <- matrix(0, p, p )
for (j in 1:p) Qmat[,j] <- rev(dmat1[j,])
Qmat <- Qmat + t(Qmat)  ## Qmat : 최종적인 n_jk matrix
cvec<-(0:(max.k-1))/tn  ## cvec : threshold c들이 모여있는 벡터
##############################
# function: gen_sim_fun, cv_mat_fun,
gen_fit <- gen_sim_fun(Gmat, Qmat)
Gmat.hat_raw = gen_fit$G
Qmat_raw = geb_fit$Q
###
# k-fold
cv_mat <-cv_mat_fun(Gmat_obs, Qmat)
k_num = 1
tmp_te = cv_mat[cv_mat[,k_num] == k_num,]
tmp_tr = cv_mat[cv_mat[,k_num] != k_num,]
cv_m = tmp_tr[,1:3]
cv_table <- cv_table_fun(cv_m)
Gmat.hat <- cv_table$G
Qmat <- cv_table$Q
## strat cv
Gmat.hat <- Gmat.hat/Qmat
Gmat.hat[!is.finite(Gmat.hat)] = 0
n = sum(Qmat)
Qpmat = Qmat/n*2
# define the variable to restore the simulation results
Result = NULL
Naive.list = list()
Result.list = list()
naive.est <- naive_BT_fun(Qpmat, Gmat.hat, p)
k = 1
# Start the algrotihm for each c (threshold variable)
# function: gen_sim_fun, cv_mat_fun,
gen_fit <- gen_sim_fun(Gmat, Qmat)
Gmat.hat_raw = gen_fit$G
Qmat_raw = gen_fit$Q
###
# k-fold
cv_mat <-cv_mat_fun(Gmat_obs, Qmat)
k_num = 1
tmp_te = cv_mat[cv_mat[,k_num] == k_num,]
tmp_tr = cv_mat[cv_mat[,k_num] != k_num,]
cv_m = tmp_tr[,1:3]
cv_table <- cv_table_fun(cv_m)
Gmat.hat <- cv_table$G
Qmat <- cv_table$Q
## strat cv
Gmat.hat <- Gmat.hat/Qmat
Gmat.hat[!is.finite(Gmat.hat)] = 0
n = sum(Qmat)
Qpmat = Qmat/n*2
# define the variable to restore the simulation results
Result = NULL
Naive.list = list()
Result.list = list()
naive.est <- naive_BT_fun(Qpmat, Gmat.hat, p)
k = 1
##############################
# function: gen_sim_fun, cv_mat_fun,
gen_fit <- gen_sim_fun(Gmat, Qmat)
Gmat.hat_raw = gen_fit$G
Qmat_raw = gen_fit$Q
###
# k-fold
cv_mat <-cv_mat_fun(Gmat_obs, Qmat)
Gmat_obs
Gmat_obs
gen_fit
Qmat
###
# k-fold
cv_mat <-cv_mat_fun(gen_fit$G, gen_fit$Q)
cv_mat
View(dmat1)
if (ii %% 10 == 0)  cat(' ',ii,'-th iteration\n')
set.seed(ii+123) ## set seed
### generating number of comparison using dmat
### output: Qmat (n_jk matrix)
dmat1 <- dmat ## dmat : {q_jk} matrix
u.idx <- which( dmat > 0) ## q_jk가 0보다 큰 index
sel.u.idx<- sample(u.idx, kn) ## d개를 sampling함.
dmat1[sel.u.idx]  <- 0  ## d개의 선택된 q_jk에는 0을 대입(어차피 해당 n_jk은 n_s로 고정할 것이므로)
dmat1 <- dmat1/sum(dmat1) ## dmat1을 prob distribution으로 만들어주기 위해 normalize.
d.sample <- drop (rmultinom(1, tn-rn*kn, prob = c(dmat1)))  ## n_jk 만들기
d.sample[sel.u.idx] <- rn ## d개의 선택된 n_jk에 n_s를 대입
dmat1 <- matrix(d.sample, p , p)  ## matrix 형태로 만들어줌.
Qmat <- matrix(0, p, p )
for (j in 1:p) Qmat[,j] <- rev(dmat1[j,])
Qmat <- Qmat + t(Qmat)  ## Qmat : 최종적인 n_jk matrix
cvec<-(0:(max.k-1))/tn  ## cvec : threshold c들이 모여있는 벡터
##############################
# function: gen_sim_fun, cv_mat_fun,
gen_fit <- gen_sim_fun(Gmat, Qmat)
Gmat.hat_raw = gen_fit$G
Qmat_raw = gen_fit$Q
cv_mat <-cv_mat_fun(gen_fit$G, gen_fit$Q)
k = 1
######### gBT model ###########
cval <- cvec[k]
# k-fold
k_num = 1
tmp_te = cv_mat[cv_mat[,k_num] == k_num,]
tmp_tr = cv_mat[cv_mat[,k_num] != k_num,]
cv_m = tmp_tr[,1:3]
cv_table <- cv_table_fun(cv_m)
Gmat.hat <- cv_table$G
Qmat <- cv_table$Q
## strat cv
Gmat.hat <- Gmat.hat/Qmat
Gmat.hat[!is.finite(Gmat.hat)] = 0
n = sum(Qmat)
Qpmat = Qmat/n*2
result <- gbt_step1_fun(Qpmat, Gmat.hat, p, cval)
result
# gbt_step2_fun
tmp<-result
not0_ind = (tmp[,1]!=0)
tmp <-tmp[not0_ind, 1:3]
p.set <-sort(unique(c(tmp[,1:2])))
(length(p.set) != p)
NA
gbt_step2_fun = function(result, p)
{
tmp<-result
not0_ind = (tmp[,1]!=0)
tmp <-tmp[not0_ind, 1:3]
p.set <-sort(unique(c(tmp[,1:2])))
if (length(p.set) != p) return(NA)
xx <- matrix(0, nrow(tmp)*2, p)
yy <- rep(0, nrow(tmp)*2)
i = 1
for ( i in 1:nrow(tmp))
{
vec1<-tmp[i,1:2]; vec2<- tmp[i,3]
xx[2*(i-1)+1, vec1] <- c(1,-1) ; yy[2*(i-1)+1] <- vec2
xx[2*i, vec1] <- c(-1,1) ; yy[2*i] <- abs(vec2 - 1)
}
xx<- xx[,-p]
fit<-glmnet(xx,yy, family = 'binomial', alpha = 0, lambda = 1e-5, intercept = FALSE,
weights = rep(Result[not0_ind,k],each=2) , standardize = F)
## weight vector v_jk는 들어가지 않나??
gbt.est <- c(fit$beta[,1],0)
cor.r <- cor(gbt.est, lambda.vec, method = 'kendall')
return(cor.r)
}
# gbt_step2_fun
gbt_step2_fun(result, p)
p
tmp<-result
not0_ind = (tmp[,1]!=0)
tmp <-tmp[not0_ind, 1:3]
p.set <-sort(unique(c(tmp[,1:2])))
if (length(p.set) != p) return(NA)
xx <- matrix(0, nrow(tmp)*2, p)
yy <- rep(0, nrow(tmp)*2)
i = 1
for ( i in 1:nrow(tmp))
{
vec1<-tmp[i,1:2]; vec2<- tmp[i,3]
xx[2*(i-1)+1, vec1] <- c(1,-1) ; yy[2*(i-1)+1] <- vec2
xx[2*i, vec1] <- c(-1,1) ; yy[2*i] <- abs(vec2 - 1)
}
xx<- xx[,-p]
fit<-glmnet(xx,yy, family = 'binomial', alpha = 0, lambda = 1e-5, intercept = FALSE,
weights = rep(Result[not0_ind,k],each=2) , standardize = F)
## weight vector v_jk는 들어가지 않나??
gbt.est <- c(fit$beta[,1],0)
xx
yy
fit<-glmnet(xx,yy, family = 'binomial', alpha = 0, lambda = 1e-5, intercept = FALSE,
weights = rep(Result[not0_ind,k],each=2) , standardize = F)
result
fit<-glmnet(xx,yy, family = 'binomial', alpha = 0, lambda = 1e-5, intercept = FALSE,
weights = rep(result[not0_ind,4],each=2) , standardize = F)
fit
## weight vector v_jk는 들어가지 않나??
gbt.est <- c(fit$beta[,1],0)
gbt.est
gbt_step2_fun = function(result, p)
{
tmp<-result
not0_ind = (tmp[,1]!=0)
tmp <-tmp[not0_ind, 1:3]
p.set <-sort(unique(c(tmp[,1:2])))
if (length(p.set) != p) return(NA)
xx <- matrix(0, nrow(tmp)*2, p)
yy <- rep(0, nrow(tmp)*2)
i = 1
for ( i in 1:nrow(tmp))
{
vec1<-tmp[i,1:2]; vec2<- tmp[i,3]
xx[2*(i-1)+1, vec1] <- c(1,-1) ; yy[2*(i-1)+1] <- vec2
xx[2*i, vec1] <- c(-1,1) ; yy[2*i] <- abs(vec2 - 1)
}
xx<- xx[,-p]
fit<-glmnet(xx,yy, family = 'binomial', alpha = 0, lambda = 1e-5, intercept = FALSE,
weights = rep(result[not0_ind, 4],each=2) , standardize = F)
## weight vector v_jk는 들어가지 않나??
gbt.est <- c(fit$beta[,1],0)
cor.r <- cor(gbt.est, lambda.vec, method = 'kendall')
return(cor.r)
}
gbt_step2_fun(result, p)
for (k_num in 1:k_fold)
{
tmp_te = cv_mat[cv_mat[,k_num] == k_num,]
tmp_tr = cv_mat[cv_mat[,k_num] != k_num,]
cv_m = tmp_tr[,1:3]
cv_table <- cv_table_fun(cv_m)
Gmat.hat <- cv_table$G
Qmat <- cv_table$Q
## strat cv
Gmat.hat <- Gmat.hat/Qmat
Gmat.hat[!is.finite(Gmat.hat)] = 0
n = sum(Qmat)
Qpmat = Qmat/n*2
result <- gbt_step1_fun(Qpmat, Gmat.hat, p, cval)
# gbt_step2_fun
cor.r<- gbt_step2_fun(result, p)
cat(cor.r,'\n')
}
k_num
cv_mat
k_num = 1
for (k_num in 1:k_fold)
{
tmp_te = cv_mat[cv_mat[,4] == k_num,]
tmp_tr = cv_mat[cv_mat[,4] != k_num,]
cv_m = tmp_tr[,1:3]
cv_table <- cv_table_fun(cv_m)
Gmat.hat <- cv_table$G
Qmat <- cv_table$Q
## strat cv
Gmat.hat <- Gmat.hat/Qmat
Gmat.hat[!is.finite(Gmat.hat)] = 0
n = sum(Qmat)
Qpmat = Qmat/n*2
result <- gbt_step1_fun(Qpmat, Gmat.hat, p, cval)
# gbt_step2_fun
cor.r<- gbt_step2_fun(result, p)
cat(cor.r,'\n')
}
k_num = 1
for (k_num in 1:k_fold)
{
tmp_te = cv_mat[cv_mat[,4] == k_num,]
tmp_tr = cv_mat[cv_mat[,4] != k_num,]
cv_m = tmp_tr[,1:3]
cv_table <- cv_table_fun(cv_m)
Gmat.hat <- cv_table$G
Qmat <- cv_table$Q
## strat cv
Gmat.hat <- Gmat.hat/Qmat
Gmat.hat[!is.finite(Gmat.hat)] = 0
n = sum(Qmat)
Qpmat = Qmat/n*2
result <- gbt_step1_fun(Qpmat, Gmat.hat, p, cval)
# gbt_step2_fun
cor.r<- gbt_step2_fun(result, p)
cat(cor.r,'\n')
}
cval
k = 3
cval <- cvec[k]
# k-fold
k_num = 1
for (k_num in 1:k_fold)
{
tmp_te = cv_mat[cv_mat[,4] == k_num,]
tmp_tr = cv_mat[cv_mat[,4] != k_num,]
cv_m = tmp_tr[,1:3]
cv_table <- cv_table_fun(cv_m)
Gmat.hat <- cv_table$G
Qmat <- cv_table$Q
## strat cv
Gmat.hat <- Gmat.hat/Qmat
Gmat.hat[!is.finite(Gmat.hat)] = 0
n = sum(Qmat)
Qpmat = Qmat/n*2
result <- gbt_step1_fun(Qpmat, Gmat.hat, p, cval)
# gbt_step2_fun
cor.r<- gbt_step2_fun(result, p)
cat(cor.r,'\n')
}
cval
k = 5
<- cvec[k]
# k-fold
k_num = 1
for (k_num in 1:k_fold)
{
tmp_te = cv_mat[cv_mat[,4] == k_num,]
tmp_tr = cv_mat[cv_mat[,4] != k_num,]
cv_m = tmp_tr[,1:3]
cv_table <- cv_table_fun(cv_m)
Gmat.hat <- cv_table$G
Qmat <- cv_table$Q
## strat cv
Gmat.hat <- Gmat.hat/Qmat
Gmat.hat[!is.finite(Gmat.hat)] = 0
n = sum(Qmat)
Qpmat = Qmat/n*2
result <- gbt_step1_fun(Qpmat, Gmat.hat, p, cval)
# gbt_step2_fun
cor.r<- gbt_step2_fun(result, p)
cat(cor.r,'\n')
}
cval <- cvec[k]
# k-fold
k_num = 1
for (k_num in 1:k_fold)
{
tmp_te = cv_mat[cv_mat[,4] == k_num,]
tmp_tr = cv_mat[cv_mat[,4] != k_num,]
cv_m = tmp_tr[,1:3]
cv_table <- cv_table_fun(cv_m)
Gmat.hat <- cv_table$G
Qmat <- cv_table$Q
## strat cv
Gmat.hat <- Gmat.hat/Qmat
Gmat.hat[!is.finite(Gmat.hat)] = 0
n = sum(Qmat)
Qpmat = Qmat/n*2
result <- gbt_step1_fun(Qpmat, Gmat.hat, p, cval)
# gbt_step2_fun
cor.r<- gbt_step2_fun(result, p)
cat(cor.r,'\n')
}
cvec
