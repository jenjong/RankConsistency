ls()
install.packages("FNN")
# Generate Train Data
set.seed(1)
x_train <- sort(rnorm(eval_n))
y_train <- 3 + x_train ^ 2 + rnorm(eval_n)
ab = 1
eval_n <- 100
# Generate Train Data
set.seed(1)
x_train <- sort(rnorm(eval_n))
y_train <- 3 + x_train ^ 2 + rnorm(eval_n)
ab = 1
k = 1
ii = 1
# Generate Train Data
set.seed(ii)
eval_n = 100
# Generate Train Data
set.seed(ii)
x_train <- sort(rnorm(eval_n))
y_train <- 3 + x_train ^ 2 + rnorm(eval_n)
yhat <- rep(0, eval_n)
knn_sig <- rep(0, 50)
# Generate Test Data
x_test <- sort(rnorm(eval_n))
# Generate Test Data
x_test <- sort(rnorm(eval_n))
y_test <- 3 + x_test ^ 2 + rnorm(eval_n)
k = 10
k = 10
# k = 10
eval_point <- x_test
idx_mat <- knnx_index(x_train, eval_point, k)
idx_mat
library(FNN)
idx_mat <- knnx.index(x_train, eval_point, k)
idx_mat
for (i in 1:eval_n)
{
yhat[i] <- mean(y_train[idx.mat[i,]])
}
idx_mat <- knnx.index(x_train, eval_point, k)
for (i in 1:eval_n)
{
yhat[i] <- mean(y_train[idx.mat[i,]])
}
for (i in 1:eval_n)
{
yhat[i] <- mean(y_train[idx_mat[i,]])
}
yhat
mean((yhat - y_test)^2)
yhat
eval_point
eval_point <- x_test
eval_point
yhat
y_test
mean((yhat - y_test)^2)
library(FNN)
ii = 1
eval_n <- 100
# Generate Train Data
set.seed(ii)
x_train <- sort(rnorm(eval_n))
y_train <- 3 + x_train ^ 2 + rnorm(eval_n)
yhat <- rep(0, eval_n)
knn_sig <- rep(0, 50)
# Generate Test Data
x_test <- sort(rnorm(eval_n))
y_test <- 3 + x_test ^ 2 + rnorm(eval_n)
for (k in 1:length(knn_sig))
{
eval_point <- x_test
idx_mat <- knnx.index(x_train, eval_point, k)
for (i in 1:eval_n)
{
yhat[i] <- mean(y_train[idx_mat[i,]])
}
knn.sig[k] <- mean((yhat - y_test)^2)
}
library(FNN)
ii = 1
eval_n <- 100
# Generate Train Data
set.seed(ii)
x_train <- sort(rnorm(eval_n))
y_train <- 3 + x_train ^ 2 + rnorm(eval_n)
yhat <- rep(0, eval_n)
knn_sig <- rep(0, 50)
# Generate Test Data
x_test <- sort(rnorm(eval_n))
y_test <- 3 + x_test ^ 2 + rnorm(eval_n)
for (k in 1:length(knn_sig))
{
eval_point <- x_test
idx_mat <- knnx.index(x_train, eval_point, k)
for (i in 1:eval_n)
{
yhat[i] <- mean(y_train[idx_mat[i,]])
}
knn_sig[k] <- mean((yhat - y_test)^2)
}
plot(knn_sig)
eval_test_n = 10000
library(FNN)
ii = 1
eval_n <- 100
# Generate Train Data
set.seed(ii)
x_train <- sort(rnorm(eval_n))
y_train <- 3 + x_train ^ 2 + rnorm(eval_n)
yhat <- rep(0, eval_n)
knn_sig <- rep(0, 50)
eval_test_n = 10000
# Generate Test Data
x_test <- sort(rnorm(eval_test_n))
y_test <- 3 + x_test ^ 2 + rnorm(eval_test_n)
for (k in 1:length(knn_sig))
{
eval_point <- x_test
idx_mat <- knnx.index(x_train, eval_point, k)
for (i in 1:eval_test_n)
{
yhat[i] <- mean(y_train[idx_mat[i,]])
}
knn_sig[k] <- mean((yhat - y_test)^2)
}
plot(knn_sig)
library(FNN)
ii = 1
eval_n <- 100
# Generate Train Data
set.seed(ii)
x_train <- sort(rnorm(eval_n))
y_train <- 3 + x_train ^ 2 + rnorm(eval_n)
yhat <- rep(0, eval_n)
knn_sig <- rep(0, 50)
eval_test_n = 100
# Generate Test Data
x_test <- sort(rnorm(eval_test_n))
y_test <- 3 + x_test ^ 2 + rnorm(eval_test_n)
for (k in 1:length(knn_sig))
{
eval_point <- x_test
idx_mat <- knnx.index(x_train, eval_point, k)
for (i in 1:eval_test_n)
{
yhat[i] <- mean(y_train[idx_mat[i,]])
}
knn_sig[k] <- mean((yhat - y_test)^2)
}
plot(knn_sig)
library(FNN)
ii = 1
eval_n <- 1000
# Generate Train Data
set.seed(ii)
x_train <- sort(rnorm(eval_n))
y_train <- 3 + x_train ^ 2 + rnorm(eval_n)
yhat <- rep(0, eval_n)
knn_sig <- rep(0, 50)
eval_test_n = 1000
# Generate Test Data
x_test <- sort(rnorm(eval_test_n))
y_test <- 3 + x_test ^ 2 + rnorm(eval_test_n)
for (k in 1:length(knn_sig))
{
eval_point <- x_test
idx_mat <- knnx.index(x_train, eval_point, k)
for (i in 1:eval_test_n)
{
yhat[i] <- mean(y_train[idx_mat[i,]])
}
knn_sig[k] <- mean((yhat - y_test)^2)
}
plot(knn_sig)
plot(knn_sig, type = 'b', color = 'lightblue')
knn_sig <- seq(1,100,by = 5)
knn_sig <- seq(1,100,by = 5)
library(FNN)
ii = 1
eval_n <- 1000
# Generate Train Data
set.seed(ii)
x_train <- sort(rnorm(eval_n))
y_train <- 3 + x_train ^ 2 + rnorm(eval_n)
yhat <- rep(0, eval_n)
knn_sig <- seq(1,100,by = 5)
eval_test_n = 1000
# Generate Test Data
x_test <- sort(rnorm(eval_test_n))
y_test <- 3 + x_test ^ 2 + rnorm(eval_test_n)
for (k in 1:length(knn_sig))
{
eval_point <- x_test
idx_mat <- knnx.index(x_train, eval_point, k)
for (i in 1:eval_test_n)
{
yhat[i] <- mean(y_train[idx_mat[i,]])
}
knn_sig[k] <- mean((yhat - y_test)^2)
}
plot(knn_sig, type = 'b', color = 'lightblue')
plot(knn_sig, type = 'b', col = 'lightblue')
knn_sig
seq(1,100, by = 5)
kvec <- seq(1,100, by = 5)
library(FNN)
ii = 1
eval_n <- 500
# Generate Train Data
set.seed(ii)
x_train <- sort(rnorm(eval_n))
y_train <- 3 + x_train ^ 2 + rnorm(eval_n)
yhat <- rep(0, eval_n)
kvec <- seq(1,100, by = 5)
error <- rep(0, length(kvec))
eval_test_n = 1000
# Generate Test Data
x_test <- sort(rnorm(eval_test_n))
y_test <- 3 + x_test ^ 2 + rnorm(eval_test_n)
for (k in 1:length(knn_sig))
{
eval_point <- x_test
idx_mat <- knnx.index(x_train, eval_point, k)
for (i in 1:eval_test_n)
{
yhat[i] <- mean(y_train[idx_mat[i,]])
}
knn_sig[k] <- mean((yhat - y_test)^2)
}
plot(knn_sig, type = 'b', col = 'lightblue')
plot(knn_sig, type = 'b', col = 'darkblue')
## description
## investigate the variance of the proposed estimator
rm(list = ls()); gc()
#setwd("E:\\rank consistency\\simulation\\code\\")
#setwd("C:/Users/uos_stat/Dropbox/A rank-consistency/prog/temp")
setwd("C:/Users/Jeon/Documents/GitHub/RankConsistency")
require('glmnet')
library(igraph)
library(ggplot2)
source('./lib/sim.R')
##########################################################################################
max.k = 10
p = 10
kn <- 10  ## d
rn <- 1   ## n_s
df = 1    ## t분포 자유도
counter = 1
sim.iter = 200    ## 전체 simulation 과정 반복 수
source('./lib/exe-2.R')
tn_vec = c(500,5000,50000)
cor.naive_list = list()
cor.r_list = list()
tn_i =1
tn = tn_vec[tn_i]  ## tn 정의 (전체 rank pair의 수.)
cat ('total n:' , tn , '\n')
cor.naive<- rep(0,sim.iter) ## BT를 이용한 kendall's tau 저장하는 벡터
cor.r <- matrix(0,sim.iter,max.k) ## gBT를 이용한 kendall's tau 저장하는 벡터
ii = 1
set.seed(ii+123) ## set seed
### generating number of comparison using dmat
### output: Qmat (n_jk matrix)
dmat1 <- dmat ## dmat : {q_jk} matrix
u.idx <- which( dmat > 0) ## q_jk가 0보다 큰 index
sel.u.idx<- sample(u.idx, kn) ## d개를 sampling함.
dmat1[sel.u.idx]  <- 0  ## d개의 선택된 q_jk에는 0을 대입(어차피 해당 n_jk은 n_s로 고정할 것이므로)
dmat1 <- dmat1/sum(dmat1) ## dmat1을 prob distribution으로 만들어주기 위해 normalize.
d.sample <- drop (rmultinom(1, tn-rn*kn, prob = c(dmat1)))  ## n_jk 만들기
d.sample[sel.u.idx] <- rn ## d개의 선택된 n_jk에 n_s를 대입
dmat1 <- matrix(d.sample, p , p)  ## matrix 형태로 만들어줌.
Qmat <- matrix(0, p, p )
for (j in 1:p) Qmat[,j] <- rev(dmat1[j,])
Qmat <- Qmat + t(Qmat)  ## Qmat : 최종적인 n_jk matrix
cvec<-(0:(max.k-1))/tn  ## cvec : threshold c들이 모여있는 벡터
cvec
dmat1
dmat
Qmat
##############################
# generating the result of win-loss ratio using Gmat(true probability matrix) on Qmat (number of matching)
# output: Gmat.hat
## Gmat.hat : true Gmat을 이용하여 data generation을 할 경우에 승패 수와 전체 대결 수를 이용하여
##            만든 Gmat의 추정값
gmat.prob<-c(Gmat)  ## Gmat_jk : j object와 k object에서 j가 k를 이길 확률.
gmat.prob
gmat.num <- c(Qmat)
gmat.gen
gmat.num <- c(Qmat)
gmat.gen<- rep(0, length(gmat.num))
for (i in 1:length(gmat.num))
{
gmat.gen[i] <- rbinom(n = 1, size = gmat.num[i], prob = gmat.prob[i])
}
gmat.gen
Gmat.hat <- matrix(gmat.gen,p,p)
Gmat.hat
Gmat.hat[lower.tri(Gmat.hat, diag = T)] = 0
Gmat.hat
tmp <- Qmat - t(Gmat.hat)
tmp
Gmat.hat[lower.tri(Qmat)]<- tmp[lower.tri(Qmat)]
Gmat.hat
Gmat.hat
Gmat.hat
Qmat
tn
n = sum(Qmat)
n
tn
cv_mat = matrix(0, tn, 2)
cumsum(Qmat)
(ncol(Gmat)-1)
s1 = 1
j = 1
i = 3
a1 = Qmat[i,j]
a1
if (a1 == 0 ) next
f_idx <- s1:(a1-1)
f_idx
f_idx <- s1:(s1+a1-1)
f_idx
cv_mat[f_idx,1]
cv_mat[f_idx,1] <- i
cv_mat[f_idx,2] <- j
cv_mat[f_idx,]
f_idx
Gmat.hat[i,j]
a2
a2 = Gmat.hat[i,j]
a2
s1 = 1
for (j in 1:(ncol(Gmat)-1))
{
for(i in (j+1):nrow(Gmat))
{
a1 = Qmat[i,j]
if (a1 == 0 ) next
f_idx <- s1:(s1+a1-1)
cv_mat[f_idx,1] <- i
cv_mat[f_idx,2] <- j
s1 = s1 + a1
a2 = Gmat.hat[i,j]
if (a2 == 0 ) next
f_idx2 <- sample(f_idx,a2)
cv_mat[f_idx,3] = 1
}
}
cv_mat = matrix(0, tn, 4)
s1 = 1
for (j in 1:(ncol(Gmat)-1))
{
for(i in (j+1):nrow(Gmat))
{
a1 = Qmat[i,j]
if (a1 == 0 ) next
f_idx <- s1:(s1+a1-1)
cv_mat[f_idx,1] <- i
cv_mat[f_idx,2] <- j
s1 = s1 + a1
a2 = Gmat.hat[i,j]
if (a2 == 0 ) next
f_idx2 <- sample(f_idx,a2)
cv_mat[f_idx,3] = 1
}
}
f_idx
Qmat
j
i
f_idx2
Gmat.hat
head(cv_mat)
cv_mat[1:100,]
tn
sample(1:k_fold, tn, replace = TRUE)
k_fold = 5
sample(1:k_fold, tn, replace = TRUE)
cv_mat[,4] <- sample(1:k_fold, tn, replace = TRUE)
cv_mat[,4]
cv_mat[,4] == 1
cv_mat[cv_mat[,4] == 1,]
cv_tr = cv_mat[cv_mat[,4] != 1,]
cv_te = cv_mat[cv_mat[,4] == 1,]
cv_tr = cv_mat[cv_mat[,4] != 1,]
cv_tr
cv_te
